{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        buffer = io.BytesIO()\n",
    "        placeholder_image.save(buffer, format='PNG')\n",
    "        buffer.seek(0)\n",
    "        async with aiofiles.open(image_save_path, 'wb') as f:\n",
    "            await f.write(buffer.getvalue())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating placeholder image: {e}\")\n",
    "\n",
    "async def download_image(session, image_link, save_folder, semaphore, retries=3, delay=1):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return\n",
    "\n",
    "    async with semaphore:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                async with session.get(image_link, timeout=10) as response:\n",
    "                    if response.status == 200:\n",
    "                        content = await response.read()\n",
    "                        async with aiofiles.open(image_save_path, 'wb') as f:\n",
    "                            await f.write(content)\n",
    "                        return\n",
    "                    else:\n",
    "                        print(f\"Failed to download {image_link}: HTTP {response.status}\")\n",
    "            except Exception as e:\n",
    "                if attempt < retries - 1:\n",
    "                    await asyncio.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"Error downloading {image_link}: {e}\")\n",
    "        \n",
    "        await create_placeholder_image(image_save_path)\n",
    "\n",
    "async def download_images(image_links, download_folder, max_concurrency=100):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [download_image(session, link, download_folder, semaphore) for link in image_links]\n",
    "        await tqdm.gather(*tasks, desc=\"Downloading images\", total=len(image_links))\n",
    "\n",
    "def run_async_download(image_links, download_folder):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # We're in an environment with an existing event loop (e.g., Jupyter)\n",
    "        asyncio.create_task(download_images(image_links, download_folder))\n",
    "    else:\n",
    "        # We're in a regular Python environment\n",
    "        loop.run_until_complete(download_images(image_links, download_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import entity_unit_map, allowed_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "s:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "s:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "s:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50.eval()\n",
    "# Define transforms for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the OCR reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# def process_image(image_path, transform, resnet50, reader):\n",
    "#     try:\n",
    "#         # Extract image features\n",
    "#         image = Image.open(image_path).convert('RGB')\n",
    "#         image_tensor = transform(image).unsqueeze(0)\n",
    "#         image_tensor = image_tensor.to(resnet50.device)\n",
    "#         with torch.no_grad():\n",
    "#             image_features = resnet50(image_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "#         # Extract text features\n",
    "#         result = reader.readtext(str(image_path))\n",
    "#         extracted_text = ' '.join([item[1] for item in result])\n",
    "\n",
    "#         return image_features, extracted_text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "#         return np.zeros(2048), \"\"\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "def extract_features(image_path):\n",
    "    # Extract image features\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        image_features = resnet50(image_tensor).squeeze().numpy()\n",
    "    \n",
    "    # Extract text features\n",
    "    result = reader.readtext(image_path)\n",
    "    extracted_text = ' '.join([item[1] for item in result])\n",
    "    \n",
    "    return image_features, extracted_text\n",
    "\n",
    "def process_dataset(df, image_folder):\n",
    "    image_features_list = []\n",
    "    extracted_texts = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        image_filename = Path(row['image_link']).name\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            image_features, extracted_text = extract_features(image_path)\n",
    "            image_features_list.append(image_features)\n",
    "            extracted_texts.append(extracted_text)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            image_features_list.append(np.zeros(2048))  # ResNet50 output size\n",
    "            extracted_texts.append(\"\")\n",
    "    \n",
    "    df['image_features'] = image_features_list\n",
    "    df['extracted_text'] = extracted_texts\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "class DeepEntityExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepEntityExtractor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_deep_model(X_train, y_train, X_val, y_val, input_dim, hidden_dim, output_dim, num_epochs=50, batch_size=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DeepEntityExtractor(input_dim, hidden_dim, output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train).to(device), torch.LongTensor(y_train).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val).to(device), torch.LongTensor(y_val).to(device))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_model_path = 'best_model.pth'\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_accuracy += (predicted == batch_y).sum().item()\n",
    "\n",
    "        val_accuracy /= len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def parse_prediction(pred, entity_name):\n",
    "    try:\n",
    "        value, unit = pred.split()\n",
    "        value = float(value)\n",
    "        if unit in entity_unit_map.get(entity_name, set()):\n",
    "            return f\"{value} {unit}\"\n",
    "    except:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "train_df = pd.read_csv('dataset/trial_train.csv')\n",
    "test_df = pd.read_csv('dataset/trial_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61I9XdN6OF...</td>\n",
       "      <td>748919</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>500.0 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71gSRbyXmo...</td>\n",
       "      <td>916768</td>\n",
       "      <td>item_volume</td>\n",
       "      <td>1.0 cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61BZ4zrjZX...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://m.media-amazon.com/images/I/612mrlqiI4...</td>\n",
       "      <td>459516</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.709 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://m.media-amazon.com/images/I/617Tl40LOX...</td>\n",
       "      <td>731432</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1400 milligram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_link  group_id  entity_name  \\\n",
       "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
       "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
       "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
       "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
       "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
       "\n",
       "     entity_value  \n",
       "0      500.0 gram  \n",
       "1         1.0 cup  \n",
       "2      0.709 gram  \n",
       "3      0.709 gram  \n",
       "4  1400 milligram  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train images...\n",
      "Downloading test images...\n",
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images: 100%|██████████| 99/99 [00:00<00:00, 131.04it/s]\n",
      "Downloading images: 100%|██████████| 256/256 [00:03<00:00, 83.73it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_download_path = 'images/train_img'\n",
    "test_download_path = 'images/test_img'\n",
    "\n",
    "image_links_train = train_df['image_link'].tolist()\n",
    "image_links_test = test_df['image_link'].tolist()\n",
    "\n",
    "print(\"Downloading train images...\")\n",
    "run_async_download(image_links_train, train_download_path)\n",
    "print(\"Downloading test images...\")\n",
    "run_async_download(image_links_test, test_download_path)\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [09:02<00:00,  2.12s/it]\n",
      "100%|██████████| 99/99 [00:24<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = process_dataset(train_df, 'images/train_img')\n",
    "test_df = process_dataset(test_df, 'images/test_img')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                          image_link  group_id  entity_name  \\\n",
      "0  https://m.media-amazon.com/images/I/61I9XdN6OF...    748919  item_weight   \n",
      "1  https://m.media-amazon.com/images/I/71gSRbyXmo...    916768  item_volume   \n",
      "2  https://m.media-amazon.com/images/I/61BZ4zrjZX...    459516  item_weight   \n",
      "3  https://m.media-amazon.com/images/I/612mrlqiI4...    459516  item_weight   \n",
      "4  https://m.media-amazon.com/images/I/617Tl40LOX...    731432  item_weight   \n",
      "\n",
      "     entity_value                                     image_features  \\\n",
      "0      500.0 gram  [-1.5245771, -2.7847428, -4.1273065, -4.761484...   \n",
      "1         1.0 cup  [-4.910853, -2.370376, -1.9847587, -4.453485, ...   \n",
      "2      0.709 gram  [-1.7896006, -1.1231364, -0.6803558, -4.866802...   \n",
      "3      0.709 gram  [-1.6998017, 1.7567462, -2.383329, -2.7924333,...   \n",
      "4  1400 milligram  [-2.1520514, 0.33016944, -3.4435117, -3.421331...   \n",
      "\n",
      "                                      extracted_text  \n",
      "0  PROPOS' NATUREJ INGREDIENT MENAGER MULTI-USAGE...  \n",
      "1  TLaeel=_ 7672 Prnne RRIFIC LEBENSMITTELECHT Cw...  \n",
      "2  COMPOSITION Serving Size: Tablet (0.709 g) Eac...  \n",
      "3  3 3 1 1 F IW! 1 5833 1 3 1 1 1 1 H 0 L 1 W # I...  \n",
      "4  Horbaach HIGH StRENGTH PSYLLIUM HUSK PLANTAGO ...  ,    index                                         image_link  group_id  \\\n",
      "0  index  https://m.media-amazon.com/images/I/110EibNycl...    156839   \n",
      "1  index  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
      "2  index  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
      "3  index  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
      "4  index  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
      "\n",
      "  entity_name                                     image_features  \\\n",
      "0      height  [-2.9597743, -2.5129561, -1.7396207, -3.185434...   \n",
      "1       width  [-1.7889788, -3.1118808, -1.263593, -2.3320825...   \n",
      "2      height  [-3.5736835, -4.169811, -1.1644479, -2.542303,...   \n",
      "3       depth  [-2.449241, -1.6178895, -1.5031168, -2.2616858...   \n",
      "4       depth  [-3.0629919, -2.4639826, -1.5982078, -2.415163...   \n",
      "\n",
      "                                      extracted_text  \n",
      "0                     3rcn 4Gin - 7te 51 44mui eetcm  \n",
      "1  Size Width Length One Size 42cm/16.54\" 200cm/7...  \n",
      "2  Size Width Length One Size 42cm/16.54\" 200cm/7...  \n",
      "3  Size Width Length One Size 42cm/16.54\" 200cm/7...  \n",
      "4  Size Width Length One Size 10.50cm/4.13\" 90cm/...  ]\n"
     ]
    }
   ],
   "source": [
    "print([train_df.head(), test_df.head()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare features and labels\n",
    "X = train_df[['extracted_text', 'entity_name', 'image_features']]\n",
    "y = train_df['entity_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_text = tfidf.fit_transform(X['extracted_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Entity name encoding\n",
    "entity_encoder = LabelEncoder()\n",
    "X_entity = entity_encoder.fit_transform(X['entity_name']).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "X_combined = np.hstack((X_text, X_entity, np.vstack(X['image_features'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test_text = tfidf.transform(test_df['extracted_text']).toarray()\n",
    "X_test_entity = np.array([entity_encoder.transform([entity]) if entity in entity_encoder.classes_ else [-1] for entity in test_df['entity_name']]).reshape(-1, 1)\n",
    "X_test_combined = np.hstack((X_test_text, X_test_entity, np.vstack(test_df['image_features'])))\n",
    "X_test_scaled = scaler.transform(X_test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Epoch 1/50, Validation Accuracy: 0.0769\n",
      "Epoch 2/50, Validation Accuracy: 0.0962\n",
      "Epoch 3/50, Validation Accuracy: 0.1346\n",
      "Epoch 4/50, Validation Accuracy: 0.1538\n",
      "Epoch 5/50, Validation Accuracy: 0.1923\n",
      "Epoch 6/50, Validation Accuracy: 0.2308\n",
      "Epoch 7/50, Validation Accuracy: 0.2692\n",
      "Epoch 8/50, Validation Accuracy: 0.2692\n",
      "Epoch 9/50, Validation Accuracy: 0.2692\n",
      "Epoch 10/50, Validation Accuracy: 0.3077\n",
      "Epoch 11/50, Validation Accuracy: 0.3077\n",
      "Epoch 12/50, Validation Accuracy: 0.3077\n",
      "Epoch 13/50, Validation Accuracy: 0.3077\n",
      "Epoch 14/50, Validation Accuracy: 0.2885\n",
      "Epoch 15/50, Validation Accuracy: 0.2885\n",
      "Epoch 16/50, Validation Accuracy: 0.2885\n",
      "Epoch 17/50, Validation Accuracy: 0.2885\n",
      "Epoch 18/50, Validation Accuracy: 0.2885\n",
      "Epoch 19/50, Validation Accuracy: 0.2885\n",
      "Epoch 20/50, Validation Accuracy: 0.2885\n",
      "Epoch 21/50, Validation Accuracy: 0.2885\n",
      "Epoch 22/50, Validation Accuracy: 0.2885\n",
      "Epoch 23/50, Validation Accuracy: 0.2885\n",
      "Epoch 24/50, Validation Accuracy: 0.2885\n",
      "Epoch 25/50, Validation Accuracy: 0.2885\n",
      "Epoch 26/50, Validation Accuracy: 0.2885\n",
      "Epoch 27/50, Validation Accuracy: 0.2885\n",
      "Epoch 28/50, Validation Accuracy: 0.2885\n",
      "Epoch 29/50, Validation Accuracy: 0.2885\n",
      "Epoch 30/50, Validation Accuracy: 0.2885\n",
      "Epoch 31/50, Validation Accuracy: 0.2885\n",
      "Epoch 32/50, Validation Accuracy: 0.2885\n",
      "Epoch 33/50, Validation Accuracy: 0.2885\n",
      "Epoch 34/50, Validation Accuracy: 0.2885\n",
      "Epoch 35/50, Validation Accuracy: 0.2885\n",
      "Epoch 36/50, Validation Accuracy: 0.2885\n",
      "Epoch 37/50, Validation Accuracy: 0.2885\n",
      "Epoch 38/50, Validation Accuracy: 0.2885\n",
      "Epoch 39/50, Validation Accuracy: 0.2885\n",
      "Epoch 40/50, Validation Accuracy: 0.2885\n",
      "Epoch 41/50, Validation Accuracy: 0.2885\n",
      "Epoch 42/50, Validation Accuracy: 0.2885\n",
      "Epoch 43/50, Validation Accuracy: 0.2885\n",
      "Epoch 44/50, Validation Accuracy: 0.2885\n",
      "Epoch 45/50, Validation Accuracy: 0.2885\n",
      "Epoch 46/50, Validation Accuracy: 0.2885\n",
      "Epoch 47/50, Validation Accuracy: 0.2885\n",
      "Epoch 48/50, Validation Accuracy: 0.2885\n",
      "Epoch 49/50, Validation Accuracy: 0.2885\n",
      "Epoch 50/50, Validation Accuracy: 0.2885\n",
      "Training fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\AppData\\Local\\Temp\\ipykernel_21864\\2922135627.py:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Validation Accuracy: 0.0588\n",
      "Epoch 2/50, Validation Accuracy: 0.1373\n",
      "Epoch 3/50, Validation Accuracy: 0.1765\n",
      "Epoch 4/50, Validation Accuracy: 0.2157\n",
      "Epoch 5/50, Validation Accuracy: 0.2157\n",
      "Epoch 6/50, Validation Accuracy: 0.2157\n",
      "Epoch 7/50, Validation Accuracy: 0.2157\n",
      "Epoch 8/50, Validation Accuracy: 0.2549\n",
      "Epoch 9/50, Validation Accuracy: 0.2549\n",
      "Epoch 10/50, Validation Accuracy: 0.2353\n",
      "Epoch 11/50, Validation Accuracy: 0.2353\n",
      "Epoch 12/50, Validation Accuracy: 0.2353\n",
      "Epoch 13/50, Validation Accuracy: 0.2353\n",
      "Epoch 14/50, Validation Accuracy: 0.2353\n",
      "Epoch 15/50, Validation Accuracy: 0.2353\n",
      "Epoch 16/50, Validation Accuracy: 0.2353\n",
      "Epoch 17/50, Validation Accuracy: 0.2353\n",
      "Epoch 18/50, Validation Accuracy: 0.2353\n",
      "Epoch 19/50, Validation Accuracy: 0.2353\n",
      "Epoch 20/50, Validation Accuracy: 0.2353\n",
      "Epoch 21/50, Validation Accuracy: 0.2353\n",
      "Epoch 22/50, Validation Accuracy: 0.2353\n",
      "Epoch 23/50, Validation Accuracy: 0.2353\n",
      "Epoch 24/50, Validation Accuracy: 0.2353\n",
      "Epoch 25/50, Validation Accuracy: 0.2353\n",
      "Epoch 26/50, Validation Accuracy: 0.2353\n",
      "Epoch 27/50, Validation Accuracy: 0.2353\n",
      "Epoch 28/50, Validation Accuracy: 0.2353\n",
      "Epoch 29/50, Validation Accuracy: 0.2353\n",
      "Epoch 30/50, Validation Accuracy: 0.2353\n",
      "Epoch 31/50, Validation Accuracy: 0.2353\n",
      "Epoch 32/50, Validation Accuracy: 0.2353\n",
      "Epoch 33/50, Validation Accuracy: 0.2353\n",
      "Epoch 34/50, Validation Accuracy: 0.2353\n",
      "Epoch 35/50, Validation Accuracy: 0.2353\n",
      "Epoch 36/50, Validation Accuracy: 0.2353\n",
      "Epoch 37/50, Validation Accuracy: 0.2353\n",
      "Epoch 38/50, Validation Accuracy: 0.2353\n",
      "Epoch 39/50, Validation Accuracy: 0.2353\n",
      "Epoch 40/50, Validation Accuracy: 0.2353\n",
      "Epoch 41/50, Validation Accuracy: 0.2353\n",
      "Epoch 42/50, Validation Accuracy: 0.2353\n",
      "Epoch 43/50, Validation Accuracy: 0.2353\n",
      "Epoch 44/50, Validation Accuracy: 0.2353\n",
      "Epoch 45/50, Validation Accuracy: 0.2353\n",
      "Epoch 46/50, Validation Accuracy: 0.2353\n",
      "Epoch 47/50, Validation Accuracy: 0.2353\n",
      "Epoch 48/50, Validation Accuracy: 0.2353\n",
      "Epoch 49/50, Validation Accuracy: 0.2353\n",
      "Epoch 50/50, Validation Accuracy: 0.2353\n",
      "Training fold 3/5\n",
      "Epoch 1/50, Validation Accuracy: 0.0392\n",
      "Epoch 2/50, Validation Accuracy: 0.0980\n",
      "Epoch 3/50, Validation Accuracy: 0.1765\n",
      "Epoch 4/50, Validation Accuracy: 0.2157\n",
      "Epoch 5/50, Validation Accuracy: 0.2157\n",
      "Epoch 6/50, Validation Accuracy: 0.2157\n",
      "Epoch 7/50, Validation Accuracy: 0.2353\n",
      "Epoch 8/50, Validation Accuracy: 0.2157\n",
      "Epoch 9/50, Validation Accuracy: 0.2157\n",
      "Epoch 10/50, Validation Accuracy: 0.2353\n",
      "Epoch 11/50, Validation Accuracy: 0.1961\n",
      "Epoch 12/50, Validation Accuracy: 0.2353\n",
      "Epoch 13/50, Validation Accuracy: 0.2157\n",
      "Epoch 14/50, Validation Accuracy: 0.2157\n",
      "Epoch 15/50, Validation Accuracy: 0.2157\n",
      "Epoch 16/50, Validation Accuracy: 0.2157\n",
      "Epoch 17/50, Validation Accuracy: 0.2353\n",
      "Epoch 18/50, Validation Accuracy: 0.2353\n",
      "Epoch 19/50, Validation Accuracy: 0.2353\n",
      "Epoch 20/50, Validation Accuracy: 0.2353\n",
      "Epoch 21/50, Validation Accuracy: 0.2353\n",
      "Epoch 22/50, Validation Accuracy: 0.2353\n",
      "Epoch 23/50, Validation Accuracy: 0.2353\n",
      "Epoch 24/50, Validation Accuracy: 0.2353\n",
      "Epoch 25/50, Validation Accuracy: 0.2353\n",
      "Epoch 26/50, Validation Accuracy: 0.2353\n",
      "Epoch 27/50, Validation Accuracy: 0.2353\n",
      "Epoch 28/50, Validation Accuracy: 0.2353\n",
      "Epoch 29/50, Validation Accuracy: 0.2353\n",
      "Epoch 30/50, Validation Accuracy: 0.2353\n",
      "Epoch 31/50, Validation Accuracy: 0.2353\n",
      "Epoch 32/50, Validation Accuracy: 0.2353\n",
      "Epoch 33/50, Validation Accuracy: 0.2353\n",
      "Epoch 34/50, Validation Accuracy: 0.2353\n",
      "Epoch 35/50, Validation Accuracy: 0.2353\n",
      "Epoch 36/50, Validation Accuracy: 0.2353\n",
      "Epoch 37/50, Validation Accuracy: 0.2353\n",
      "Epoch 38/50, Validation Accuracy: 0.2353\n",
      "Epoch 39/50, Validation Accuracy: 0.2353\n",
      "Epoch 40/50, Validation Accuracy: 0.2353\n",
      "Epoch 41/50, Validation Accuracy: 0.2353\n",
      "Epoch 42/50, Validation Accuracy: 0.2353\n",
      "Epoch 43/50, Validation Accuracy: 0.2353\n",
      "Epoch 44/50, Validation Accuracy: 0.2353\n",
      "Epoch 45/50, Validation Accuracy: 0.2353\n",
      "Epoch 46/50, Validation Accuracy: 0.2353\n",
      "Epoch 47/50, Validation Accuracy: 0.2353\n",
      "Epoch 48/50, Validation Accuracy: 0.2353\n",
      "Epoch 49/50, Validation Accuracy: 0.2353\n",
      "Epoch 50/50, Validation Accuracy: 0.2353\n",
      "Training fold 4/5\n",
      "Epoch 1/50, Validation Accuracy: 0.0980\n",
      "Epoch 2/50, Validation Accuracy: 0.1569\n",
      "Epoch 3/50, Validation Accuracy: 0.2353\n",
      "Epoch 4/50, Validation Accuracy: 0.2353\n",
      "Epoch 5/50, Validation Accuracy: 0.2549\n",
      "Epoch 6/50, Validation Accuracy: 0.2941\n",
      "Epoch 7/50, Validation Accuracy: 0.2745\n",
      "Epoch 8/50, Validation Accuracy: 0.2549\n",
      "Epoch 9/50, Validation Accuracy: 0.2549\n",
      "Epoch 10/50, Validation Accuracy: 0.3137\n",
      "Epoch 11/50, Validation Accuracy: 0.3137\n",
      "Epoch 12/50, Validation Accuracy: 0.3333\n",
      "Epoch 13/50, Validation Accuracy: 0.3137\n",
      "Epoch 14/50, Validation Accuracy: 0.3333\n",
      "Epoch 15/50, Validation Accuracy: 0.3137\n",
      "Epoch 16/50, Validation Accuracy: 0.2941\n",
      "Epoch 17/50, Validation Accuracy: 0.3137\n",
      "Epoch 18/50, Validation Accuracy: 0.3137\n",
      "Epoch 19/50, Validation Accuracy: 0.3137\n",
      "Epoch 20/50, Validation Accuracy: 0.3137\n",
      "Epoch 21/50, Validation Accuracy: 0.3137\n",
      "Epoch 22/50, Validation Accuracy: 0.3137\n",
      "Epoch 23/50, Validation Accuracy: 0.3137\n",
      "Epoch 24/50, Validation Accuracy: 0.3137\n",
      "Epoch 25/50, Validation Accuracy: 0.3137\n",
      "Epoch 26/50, Validation Accuracy: 0.3137\n",
      "Epoch 27/50, Validation Accuracy: 0.3137\n",
      "Epoch 28/50, Validation Accuracy: 0.3137\n",
      "Epoch 29/50, Validation Accuracy: 0.3137\n",
      "Epoch 30/50, Validation Accuracy: 0.3137\n",
      "Epoch 31/50, Validation Accuracy: 0.3137\n",
      "Epoch 32/50, Validation Accuracy: 0.3137\n",
      "Epoch 33/50, Validation Accuracy: 0.3137\n",
      "Epoch 34/50, Validation Accuracy: 0.3137\n",
      "Epoch 35/50, Validation Accuracy: 0.3137\n",
      "Epoch 36/50, Validation Accuracy: 0.3137\n",
      "Epoch 37/50, Validation Accuracy: 0.3137\n",
      "Epoch 38/50, Validation Accuracy: 0.3137\n",
      "Epoch 39/50, Validation Accuracy: 0.3137\n",
      "Epoch 40/50, Validation Accuracy: 0.3137\n",
      "Epoch 41/50, Validation Accuracy: 0.3137\n",
      "Epoch 42/50, Validation Accuracy: 0.3137\n",
      "Epoch 43/50, Validation Accuracy: 0.3137\n",
      "Epoch 44/50, Validation Accuracy: 0.3137\n",
      "Epoch 45/50, Validation Accuracy: 0.3137\n",
      "Epoch 46/50, Validation Accuracy: 0.3137\n",
      "Epoch 47/50, Validation Accuracy: 0.3137\n",
      "Epoch 48/50, Validation Accuracy: 0.3137\n",
      "Epoch 49/50, Validation Accuracy: 0.3137\n",
      "Epoch 50/50, Validation Accuracy: 0.3137\n",
      "Training fold 5/5\n",
      "Epoch 1/50, Validation Accuracy: 0.0980\n",
      "Epoch 2/50, Validation Accuracy: 0.0588\n",
      "Epoch 3/50, Validation Accuracy: 0.1176\n",
      "Epoch 4/50, Validation Accuracy: 0.1961\n",
      "Epoch 5/50, Validation Accuracy: 0.2157\n",
      "Epoch 6/50, Validation Accuracy: 0.2353\n",
      "Epoch 7/50, Validation Accuracy: 0.2745\n",
      "Epoch 8/50, Validation Accuracy: 0.2941\n",
      "Epoch 9/50, Validation Accuracy: 0.2941\n",
      "Epoch 10/50, Validation Accuracy: 0.3137\n",
      "Epoch 11/50, Validation Accuracy: 0.3137\n",
      "Epoch 12/50, Validation Accuracy: 0.3333\n",
      "Epoch 13/50, Validation Accuracy: 0.3529\n",
      "Epoch 14/50, Validation Accuracy: 0.3333\n",
      "Epoch 15/50, Validation Accuracy: 0.2941\n",
      "Epoch 16/50, Validation Accuracy: 0.2745\n",
      "Epoch 17/50, Validation Accuracy: 0.3137\n",
      "Epoch 18/50, Validation Accuracy: 0.3137\n",
      "Epoch 19/50, Validation Accuracy: 0.3333\n",
      "Epoch 20/50, Validation Accuracy: 0.3333\n",
      "Epoch 21/50, Validation Accuracy: 0.3333\n",
      "Epoch 22/50, Validation Accuracy: 0.3333\n",
      "Epoch 23/50, Validation Accuracy: 0.3333\n",
      "Epoch 24/50, Validation Accuracy: 0.3333\n",
      "Epoch 25/50, Validation Accuracy: 0.3333\n",
      "Epoch 26/50, Validation Accuracy: 0.3333\n",
      "Epoch 27/50, Validation Accuracy: 0.3333\n",
      "Epoch 28/50, Validation Accuracy: 0.3333\n",
      "Epoch 29/50, Validation Accuracy: 0.3333\n",
      "Epoch 30/50, Validation Accuracy: 0.3333\n",
      "Epoch 31/50, Validation Accuracy: 0.3333\n",
      "Epoch 32/50, Validation Accuracy: 0.3333\n",
      "Epoch 33/50, Validation Accuracy: 0.3333\n",
      "Epoch 34/50, Validation Accuracy: 0.3333\n",
      "Epoch 35/50, Validation Accuracy: 0.3333\n",
      "Epoch 36/50, Validation Accuracy: 0.3333\n",
      "Epoch 37/50, Validation Accuracy: 0.3333\n",
      "Epoch 38/50, Validation Accuracy: 0.3333\n",
      "Epoch 39/50, Validation Accuracy: 0.3333\n",
      "Epoch 40/50, Validation Accuracy: 0.3333\n",
      "Epoch 41/50, Validation Accuracy: 0.3333\n",
      "Epoch 42/50, Validation Accuracy: 0.3333\n",
      "Epoch 43/50, Validation Accuracy: 0.3333\n",
      "Epoch 44/50, Validation Accuracy: 0.3333\n",
      "Epoch 45/50, Validation Accuracy: 0.3333\n",
      "Epoch 46/50, Validation Accuracy: 0.3333\n",
      "Epoch 47/50, Validation Accuracy: 0.3333\n",
      "Epoch 48/50, Validation Accuracy: 0.3333\n",
      "Epoch 49/50, Validation Accuracy: 0.3333\n",
      "Epoch 50/50, Validation Accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "deep_predictions = np.zeros((len(X_test_scaled), len(le.classes_)))\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_scaled, y_encoded)):\n",
    "    print(f\"Training fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_encoded[train_index], y_encoded[val_index]\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    hidden_dim = 256\n",
    "    output_dim = len(le.classes_)\n",
    "\n",
    "    model = train_deep_model(X_train, y_train, X_val, y_val, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    # Predict on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.FloatTensor(X_test_scaled).to(device))\n",
    "        deep_predictions += torch.softmax(test_outputs, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_predictions /= n_splits\n",
    "y_pred_encoded = np.argmax(deep_predictions, axis=1)\n",
    "y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '21' '22' '23'\n",
      " '24' '25' '26' '27' '28' '29' '30' '31' '32' '33' '34' '35' '36' '37'\n",
      " '38' '39' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '50' '51'\n",
      " '52' '53' '54' '55' '56' '57' '58' '59' '60' '61' '62' '63' '64' '65'\n",
      " '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76' '77' '78' '79'\n",
      " '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91' '92' '93'\n",
      " '94' '95' '96' '97' '98']\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any non-numeric values in the 'index' column\n",
    "print(test_df['index'].unique())  # This will show you any problematic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows where 'index' is not a valid number\n",
    "test_df = test_df[test_df['index'].apply(lambda x: str(x).isdigit())]  # Keep only rows with numeric 'index'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now convert 'index' to int safely\n",
    "test_df['index'] = test_df['index'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process predictions\n",
    "final_predictions = []\n",
    "for pred, entity_name in zip(y_pred, test_df['entity_name']):\n",
    "    final_predictions.append(parse_prediction(pred, entity_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'y_pred' is in the correct format as well\n",
    "y_pred = pd.Series(y_pred, name='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'index': test_df['index'],\n",
    "    'prediction': y_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output\n",
    "output_df.to_csv('test_out.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
